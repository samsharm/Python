{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_MNIST.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rwJAl8otL9Ol"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"180\" /></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NRRJxiMRL9Op"},"source":["### Table of Content\n","\n","1. [MNIST Overview](#section1)<br>\n","2. [MNIST using Keras](#section2)<br>\n","3. [MNIST using Eager Execution](#section3)<br>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7Lh_FSh0L9Or"},"source":["<a id=section1></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mJXCoAlRL9Ot"},"source":["# MNIST Handwritten Digits Dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iGCRUiZsL9Oz"},"source":["* MNIST - __Dataset__ for evaluating machine learning models __on the handwritten digit classification problem.__\n","* Contains __28 x 28 sized images__ which are __normalized and centred.__\n","* __60,000 images__ are used to __train a model__ and a separate set of __10,000 images are used to test it.__"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F5_9rxqkL9O0"},"source":["<a id=section2></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WW8jWfaoL9O3"},"source":["# MNIST Using Keras"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AWdS_KrPL9O4"},"source":["Let's tackle the MNIST dataset using Keras - "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Rx7PGOMWL9O6"},"source":["## 1. Import the Libraries"]},{"cell_type":"code","metadata":{"id":"sTActDr-wSt8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1579503757365,"user_tz":-330,"elapsed":2535,"user":{"displayName":"Mohit Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWeTZUrbhhCiZ-3HbrA91YVJTlW3olUgVSeOr8Ww=s64","userId":"17943522262909727589"}},"outputId":"5d37ef12-3663-4912-9185-c309394c80c4"},"source":["# Import tensorflow 2.x\n","# This code block will only work in Google Colab.\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R0nlb6VgzriU","colab":{}},"source":["import numpy as np\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hygB-9bAL9PZ"},"source":["## 2. Set Random Seed for Reproducibility"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rX7j4LWfL9Pa","colab":{}},"source":["seed = 7\n","np.random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zmuVBVOQL9Pd"},"source":["## 3. Load MNIST\n","\n","Keras provides very __convenient means of loading the dataset__ as well as doing the __data slicing__ as shown below.\n","\n","Note - It is customary to name the attributes X (matrix) in upper-case and the label y (vector) in lower-case."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ogbUarREL9Pe","colab":{}},"source":["# load (downloaded if needed) the MNIST dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yqI0EvVlL9Pj"},"source":["## 4. Manually Observe the Dataset\n","\n","__Plot a few images__ to see how the dataset actually looks like - "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D6wZM1EfL9Pk","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.close('all')\n","plt.subplot(221)                                             # Used to plot more than one figure in a graph\n","plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n","plt.subplot(222)\n","plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n","plt.subplot(223)\n","plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n","plt.subplot(224)\n","plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n","# show the plot\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DeNkae_UL9Pt"},"source":["## 5. Flatten Images\n","\n","The training dataset is structured as a __3-D array of images, image width and image height__ - "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JVfPJbs2L9Px"},"source":["* Since our network will be a __simple MLP, we need a vectorized input.__\n","* For this, we __flatten 28 x 28 pixels into 784 linear values.__\n","* For instance, in this diagram, we flatten a 3x3 image matrix into a vector of 9 values -"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fQRQ3FQdL9P0","colab":{}},"source":["# flatten 28*28 images to a 784 vector for each image\n","num_pixels = X_train.shape[1] * X_train.shape[2]\n","test_images = X_test\n","X_train = X_train.reshape(X_train.shape[0], num_pixels)\n","X_test = X_test.reshape(X_test.shape[0], num_pixels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mWmjpyUDL9P7"},"source":["## 6. Normalize Features and One-Hot Encode Labels\n","\n","* Normalization is almost always needed for __faster convergence__ and __scale all attributes to a comparable range.__\n","* Here, __each of the 784 pixels is an attribute.__\n","* Since a gray pixel value varies from __0-255, dividing by 255 is a good method for normalizing.__"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lIuFIk9xL9P8","colab":{}},"source":["# normalize inputs from 0-255 to 0-1\n","X_train = X_train / 255\n","X_test = X_test / 255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UqRDDZdYL9P_"},"source":["Converting labels to categorical one-hot arrays - "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8PqRFgWeL9QA","colab":{}},"source":["# one hot encode outputs\n","y_train = to_categorical(y_train)\n","test_labels = y_test\n","y_test = to_categorical(y_test)\n","num_classes = y_test.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p2gDFJ-kL9QH"},"source":["## 7. Define Neural Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cE6dStrbL9QJ"},"source":["We define a simple Neural Network with __784 inputs units, two hidden layers and one output layer -__"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DJib-EufL9QM","colab":{}},"source":["# create model\n","model = Sequential()\n","model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n","model.add(Dense(num_pixels, kernel_initializer='normal', activation='relu'))\n","model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n","# Compile model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EZRfz6n-L9QT"},"source":["## 8. Fit the Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e9PnuFBAL9QV","colab":{}},"source":["# Fit the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hEgR1xXzL9Qj"},"source":["After our model has trained, it will learn to predict digits in this manner - "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ed2UMagEL9Ql"},"source":["<center><img src=\"https://thumbs.gfycat.com/WeepyConcreteGemsbok-size_restricted.gif\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","collapsed":true,"id":"wPNFAAH3L9Qn"},"source":["## 9. Evaluate Performance"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1aoz-HJVL9Qo"},"source":["Finally, we evaluate the model by __looking at the correctly and incorrectly__ classified images"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z4GZGSnWL9Qq","colab":{}},"source":["loss_and_metrics = model.evaluate(X_test, y_test, verbose=2)\n","\n","print(\"Test Loss\", loss_and_metrics[0])\n","print(\"Test Accuracy\", loss_and_metrics[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B75FnRaML9Qt","scrolled":true,"colab":{}},"source":["#take ith image\n","i=0\n","predictions = model.predict(X_test)\n","print (predictions[i])\n","print (\"Model predicts {}, for the actual digit {}\".format(np.argmax(predictions[i]), np.argmax(y_test[i])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rcVsWM6gL9Qv","colab":{}},"source":["#check incorrect predictions in a range\n","predicted_classes = model.predict_classes(X_test)\n","for i in range (1000):\n","    if(predicted_classes[i] != test_labels[i]) :\n","        print (\"Model predicts {}, for the actual digit {}\".format(np.argmax(predictions[i]), np.argmax(y_test[i])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pOYIp2HxL9Q0"},"source":["<a id=section3></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yYHJctBDL9Q1"},"source":["# MNIST Using TensorFlow Eager "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qb7lvy5GL9Q4"},"source":["* Eager execution can be used for __rapid build and checks.__\n","* However, using TensorFlow instead of Keras means we need to __declare our own loss and accuracy functions.__\n","\n","Note - Restart Kernel before running Eager, it needs to be instantiated at the beginning."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"V8RlgUrPL9Q5"},"source":["## 1. Import the Libraries"]},{"cell_type":"markdown","metadata":{"id":"J0H1EC0REQZD","colab_type":"text"},"source":["- Using **TensorFlow 1**.\n","\n","- The next cell will only work on **Google Colab**, because it allows us to choose between the 1.x or 2.x versions of TensorFlow."]},{"cell_type":"code","metadata":{"id":"1gz37u7UEJ4B","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G783iC-1L9Q6","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cF_YUZ9VEH9P","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TY2tfE9lL9RE"},"source":["## 2. Enable Eager Execution"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pwa1tB89L9RG","colab":{}},"source":["# Set Eager API\n","tf.enable_eager_execution()\n","tfe = tf.contrib.eager"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0jGXPbq9L9RJ"},"source":["## 3. Load the Data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H7aFE5EzL9RL","scrolled":true,"colab":{},"outputId":"743b80db-a347-4089-b11a-d13720394109"},"source":["# Import MNIST data\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Extracting /tmp/data/train-images-idx3-ubyte.gz\n","Extracting /tmp/data/train-labels-idx1-ubyte.gz\n","Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n","Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yEIzcZChL9RR"},"source":["## 4. Initialize Hyperparameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bnqTQAsyL9RS","colab":{}},"source":["# Parameters\n","learning_rate = 0.001\n","num_steps = 10000\n","batch_size = 128\n","display_step = 100\n","\n","# Network Parameters\n","n_hidden_1 = 256 # 1st layer number of neurons\n","n_hidden_2 = 256 # 2nd layer number of neurons\n","num_input = 784 # MNIST data input (img shape: 28*28)\n","num_classes = 10 # MNIST total classes (0-9 digits)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RMLUlcrrL9RY"},"source":["## 5. Make Eager Iterator for Dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5xdUbdXbL9RZ","colab":{}},"source":["# Iterator for the dataset\n","dataset = tf.data.Dataset.from_tensor_slices(\n","    (mnist.train.images, mnist.train.labels))\n","dataset = dataset.repeat().batch(batch_size).prefetch(batch_size)\n","dataset_iter = tfe.Iterator(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mBd3j1mWL9Rc"},"source":["## 6. Define Neural Network"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kb8ucb5vL9Rd","colab":{},"outputId":"1ff662ee-1f35-40fd-b097-1f15471c0512"},"source":["# Define the neural network. To use eager API and tf.layers API together,\n","# we must instantiate a tfe.Network class as follow:\n","class NeuralNet(tfe.Network):\n","    def __init__(self):\n","        # Define each layer\n","        super(NeuralNet, self).__init__()\n","        # Hidden fully connected layer with 256 neurons\n","        self.layer1 = self.track_layer(\n","            tf.layers.Dense(n_hidden_1, activation=tf.nn.relu))\n","        # Hidden fully connected layer with 256 neurons\n","        self.layer2 = self.track_layer(\n","            tf.layers.Dense(n_hidden_2, activation=tf.nn.relu))\n","        # Output fully connected layer with a neuron for each class\n","        self.out_layer = self.track_layer(tf.layers.Dense(num_classes))\n","\n","    def call(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        return self.out_layer(x)\n","\n","\n","neural_net = NeuralNet()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:** tfe.Network is deprecated and will be removed in a future version.\n","\n","Please inherit from `tf.keras.Model`, and see its documentation for details. `tf.keras.Model` should be a drop-in replacement for `tfe.Network` in most cases, but note that `track_layer` is no longer necessary or supported. Instead, `Layer` instances are tracked on attribute assignment (see the section of `tf.keras.Model`'s documentation on subclassing). Since the output of `track_layer` is often assigned to an attribute anyway, most code can be ported by simply removing the `track_layer` calls.\n","\n","`tf.keras.Model` works with all TensorFlow `Layer` instances, including those from `tf.layers`, but switching to the `tf.keras.layers` versions along with the migration to `tf.keras.Model` is recommended, since it will preserve variable names. Feel free to import it with an alias to avoid excess typing :).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","collapsed":true,"id":"WrGSCGzDL9Ro"},"source":["## 7. Specify Loss and Accuracy Functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DSwDLkWSL9Rp","colab":{}},"source":["# Cross-Entropy loss function\n","def loss_fn(inference_fn, inputs, labels):\n","    # Using sparse_softmax cross entropy\n","    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","        logits=inference_fn(inputs), labels=labels))\n","\n","# Calculate accuracy\n","def accuracy_fn(inference_fn, inputs, labels):\n","    prediction = tf.nn.softmax(inference_fn(inputs))\n","    correct_pred = tf.equal(tf.argmax(prediction, 1), labels)\n","    return tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","\n","# SGD Optimizer\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n","\n","# Compute gradients\n","grad = tfe.implicit_gradients(loss_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iIiORxJuL9Rs"},"source":["## 8. Train the Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OxVhjPgLL9Rt","scrolled":true,"colab":{},"outputId":"23bc4b3d-5231-4c0e-8d6d-6716135b8b49"},"source":["# Training\n","average_loss = 0.\n","average_acc = 0.\n","for step in range(num_steps):\n","\n","    # Iterate through the dataset\n","    d = dataset_iter.next()\n","\n","    # Images\n","    x_batch = d[0]\n","    # Labels\n","    y_batch = tf.cast(d[1], dtype=tf.int64)\n","\n","    # Compute the batch loss\n","    batch_loss = loss_fn(neural_net, x_batch, y_batch)\n","    average_loss += batch_loss\n","    # Compute the batch accuracy\n","    batch_accuracy = accuracy_fn(neural_net, x_batch, y_batch)\n","    average_acc += batch_accuracy\n","\n","    if step == 0:\n","        # Display the initial cost, before optimizing\n","        print(\"Initial loss= {:.9f}\".format(average_loss))\n","\n","    # Update the variables following gradients info\n","    optimizer.apply_gradients(grad(neural_net, x_batch, y_batch))\n","\n","    # Display info\n","    if (step + 1) % display_step == 0 or step == 0:\n","        if step > 0:\n","            average_loss /= display_step\n","            average_acc /= display_step\n","        print(\"Step:\", '%04d' % (step + 1), \" loss=\",\n","              \"{:.9f}\".format(average_loss), \" accuracy=\",\n","              \"{:.4f}\".format(average_acc))\n","        average_loss = 0.\n","        average_acc = 0."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Initial loss= 2.345647335\n","Step: 0001  loss= 2.345647335  accuracy= 0.0781\n","Step: 0100  loss= 2.274475336  accuracy= 0.1259\n","Step: 0200  loss= 2.230574608  accuracy= 0.2231\n","Step: 0300  loss= 2.170125008  accuracy= 0.3077\n","Step: 0400  loss= 2.111117840  accuracy= 0.4073\n","Step: 0500  loss= 2.038504839  accuracy= 0.5015\n","Step: 0600  loss= 1.983657122  accuracy= 0.5513\n","Step: 0700  loss= 1.921869159  accuracy= 0.5946\n","Step: 0800  loss= 1.863393307  accuracy= 0.6259\n","Step: 0900  loss= 1.771777630  accuracy= 0.6801\n","Step: 1000  loss= 1.718756676  accuracy= 0.6741\n","Step: 1100  loss= 1.663859725  accuracy= 0.6901\n","Step: 1200  loss= 1.586413980  accuracy= 0.7123\n","Step: 1300  loss= 1.505777597  accuracy= 0.7384\n","Step: 1400  loss= 1.462781787  accuracy= 0.7294\n","Step: 1500  loss= 1.395831466  accuracy= 0.7437\n","Step: 1600  loss= 1.339035392  accuracy= 0.7602\n","Step: 1700  loss= 1.276643872  accuracy= 0.7653\n","Step: 1800  loss= 1.204934478  accuracy= 0.7790\n","Step: 1900  loss= 1.168129086  accuracy= 0.7791\n","Step: 2000  loss= 1.138081074  accuracy= 0.7842\n","Step: 2100  loss= 1.095602989  accuracy= 0.7874\n","Step: 2200  loss= 0.998635471  accuracy= 0.8202\n","Step: 2300  loss= 1.010611653  accuracy= 0.7954\n","Step: 2400  loss= 0.979707122  accuracy= 0.8084\n","Step: 2500  loss= 0.945891559  accuracy= 0.8065\n","Step: 2600  loss= 0.871745825  accuracy= 0.8341\n","Step: 2700  loss= 0.878102124  accuracy= 0.8153\n","Step: 2800  loss= 0.863124073  accuracy= 0.8185\n","Step: 2900  loss= 0.828474939  accuracy= 0.8324\n","Step: 3000  loss= 0.783382416  accuracy= 0.8373\n","Step: 3100  loss= 0.771250904  accuracy= 0.8311\n","Step: 3200  loss= 0.750428975  accuracy= 0.8390\n","Step: 3300  loss= 0.768579781  accuracy= 0.8348\n","Step: 3400  loss= 0.728799582  accuracy= 0.8366\n","Step: 3500  loss= 0.670978248  accuracy= 0.8570\n","Step: 3600  loss= 0.698173285  accuracy= 0.8402\n","Step: 3700  loss= 0.699491739  accuracy= 0.8441\n","Step: 3800  loss= 0.679581165  accuracy= 0.8425\n","Step: 3900  loss= 0.620164096  accuracy= 0.8638\n","Step: 4000  loss= 0.642308116  accuracy= 0.8487\n","Step: 4100  loss= 0.649513841  accuracy= 0.8473\n","Step: 4200  loss= 0.629175186  accuracy= 0.8539\n","Step: 4300  loss= 0.581851482  accuracy= 0.8661\n","Step: 4400  loss= 0.612513602  accuracy= 0.8507\n","Step: 4500  loss= 0.589275181  accuracy= 0.8591\n","Step: 4600  loss= 0.595017850  accuracy= 0.8616\n","Step: 4700  loss= 0.578213453  accuracy= 0.8580\n","Step: 4800  loss= 0.544852257  accuracy= 0.8706\n","Step: 4900  loss= 0.561857581  accuracy= 0.8614\n","Step: 5000  loss= 0.571524918  accuracy= 0.8628\n","Step: 5100  loss= 0.558679819  accuracy= 0.8611\n","Step: 5200  loss= 0.501891434  accuracy= 0.8824\n","Step: 5300  loss= 0.541800857  accuracy= 0.8613\n","Step: 5400  loss= 0.547557354  accuracy= 0.8652\n","Step: 5500  loss= 0.526719809  accuracy= 0.8695\n","Step: 5600  loss= 0.486128658  accuracy= 0.8813\n","Step: 5700  loss= 0.530752242  accuracy= 0.8640\n","Step: 5800  loss= 0.518310368  accuracy= 0.8684\n","Step: 5900  loss= 0.506323874  accuracy= 0.8770\n","Step: 6000  loss= 0.495424449  accuracy= 0.8714\n","Step: 6100  loss= 0.478675842  accuracy= 0.8795\n","Step: 6200  loss= 0.482337564  accuracy= 0.8775\n","Step: 6300  loss= 0.503762007  accuracy= 0.8726\n","Step: 6400  loss= 0.498268694  accuracy= 0.8688\n","Step: 6500  loss= 0.435280114  accuracy= 0.8930\n","Step: 6600  loss= 0.484769851  accuracy= 0.8718\n","Step: 6700  loss= 0.484422535  accuracy= 0.8765\n","Step: 6800  loss= 0.481477588  accuracy= 0.8732\n","Step: 6900  loss= 0.426786005  accuracy= 0.8931\n","Step: 7000  loss= 0.472034097  accuracy= 0.8761\n","Step: 7100  loss= 0.470690131  accuracy= 0.8776\n","Step: 7200  loss= 0.458837688  accuracy= 0.8827\n","Step: 7300  loss= 0.429651767  accuracy= 0.8866\n","Step: 7400  loss= 0.452631146  accuracy= 0.8813\n","Step: 7500  loss= 0.438716918  accuracy= 0.8865\n","Step: 7600  loss= 0.460072517  accuracy= 0.8802\n","Step: 7700  loss= 0.446488112  accuracy= 0.8803\n","Step: 7800  loss= 0.406748682  accuracy= 0.8963\n","Step: 7900  loss= 0.433683813  accuracy= 0.8858\n","Step: 8000  loss= 0.449546397  accuracy= 0.8821\n","Step: 8100  loss= 0.443661511  accuracy= 0.8804\n","Step: 8200  loss= 0.398742378  accuracy= 0.8992\n","Step: 8300  loss= 0.426778764  accuracy= 0.8876\n","Step: 8400  loss= 0.438585967  accuracy= 0.8819\n","Step: 8500  loss= 0.428647757  accuracy= 0.8864\n","Step: 8600  loss= 0.391403228  accuracy= 0.8980\n","Step: 8700  loss= 0.428689510  accuracy= 0.8848\n","Step: 8800  loss= 0.416640878  accuracy= 0.8885\n","Step: 8900  loss= 0.419081450  accuracy= 0.8888\n","Step: 9000  loss= 0.412739217  accuracy= 0.8877\n","Step: 9100  loss= 0.394084752  accuracy= 0.8958\n","Step: 9200  loss= 0.401487797  accuracy= 0.8936\n","Step: 9300  loss= 0.419081956  accuracy= 0.8855\n","Step: 9400  loss= 0.420295805  accuracy= 0.8846\n","Step: 9500  loss= 0.360949665  accuracy= 0.9080\n","Step: 9600  loss= 0.406582385  accuracy= 0.8905\n","Step: 9700  loss= 0.412491918  accuracy= 0.8871\n","Step: 9800  loss= 0.404040694  accuracy= 0.8890\n","Step: 9900  loss= 0.367442518  accuracy= 0.9034\n","Step: 10000  loss= 0.405036420  accuracy= 0.8901\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","collapsed":true,"id":"HCeOUCWJL9R7"},"source":["## 9. Test the Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TTjmrTEpL9R9","colab":{},"outputId":"bef21fc3-ff8c-4d11-c7a5-abab3c374ceb"},"source":["# Evaluate model on the test image set\n","testX = mnist.test.images\n","testY = mnist.test.labels\n","\n","test_acc = accuracy_fn(neural_net, testX, testY)\n","print(\"Testset Accuracy: {:.4f}\".format(test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testset Accuracy: 0.9010\n"],"name":"stdout"}]}]}