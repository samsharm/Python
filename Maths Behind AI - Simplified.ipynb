{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"307.2px"},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Maths Behind AI - Simplified.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"qaDsK9atlIfo"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"110\" /></center>\n","\n","# <center>Maths Behind AI - Simplified</center>"]},{"cell_type":"markdown","metadata":{"id":"vCYGAYXflIfq"},"source":["## Table of Contents\n","\n","1. [Data Types Used in AI](#section1)<br>\n","  - 1.1 [Scalars (0D Tensors)](#section101)<br>\n","  - 1.2 [Vectors (1D Tensors)](#section102)<br>\n","  - 1.3 [Matrices (2D Tensors)](#section103)<br>\n","  - 1.4 [3D Tensors and Higher-Dimensional Tensors](#section104)<br>\n","  - 1.5 [Key attributes of Tensors](#section105)<br>\n","  - 1.6 [Manipulating tensors in Numpy](#section106)<br>\n","  - 1.7 [The Notion of Data Batches](#section107)<br>\n","  - 1.8 [Real-world Examples of Data Tensors](#section108)<br><br>\n","2. [Vector Data](#section2)<br><br>\n","3. [Tensor Operations in a Nutshell](#section3)<br>\n","  - 3.1 [Element-wise Operations](#section301)<br>\n","  - 3.2 [Broadcasting](#section302)<br>\n","  - 3.3 [Tensor Dot](#section303)<br>\n","  - 3.4 [Tensor Reshaping](#section304)<br>\n","  - 3.5 [A Geometric Interpretation of Deep Learning](#section305)<br><br>\n","4. [Basic Maths for Gradient Descent](#section4)<br>\n","  - 4.1 [What’s a Derivative?](#section401)<br>\n","  - 4.2 [Derivative of a Tensor Operation: the Gradient](#section402)<br>\n","  - 4.3 [Stochastic Gradient Descent](#section403)<br>\n","  - 4.4 [Chaining Derivatives: the Backpropagation Algorithm](#section404)<br>"]},{"cell_type":"markdown","metadata":{"id":"zIwpTvvuGgzS"},"source":["**Note**: For the best visual experience, please run this notebook on Google Colab."]},{"cell_type":"markdown","metadata":{"id":"r0X_O-oBlIfr"},"source":["<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/title.jpg\" width=\"500\" height=\"300\"/></center>\r\n"]},{"cell_type":"markdown","metadata":{"id":"7i_ujMY9lIft"},"source":["<a id=section1></a>\n","## 1. Data Types Used in AI"]},{"cell_type":"markdown","metadata":{"id":"oHp9xSL5lIfv"},"source":["- **Tensors**: Data stored in **multidimensional Numpy arrays** are called **tensors**. \n","\n","\n","- In general, all current machine-learning systems **use tensors** as their **basic data structure**. \n","\n","\n","- Tensors are **fundamental** to the **field**—so fundamental that Google’s TensorFlow was named after them. \n","\n","\n","- So what’s a tensor? At its core, **a tensor is a container for data**—almost always **numerical** data. So, it’s a **container for numbers**. \n","\n","\n","- You may be already familiar with **matrices**, which are **2D** tensors:\n","\n","  - **Tensors** are a **generalization** of **matrices** to an **arbitrary** number of **dimensions**.\n","  \n","  - **Note**: In the context of tensors, a **dimension** is often called an **axis**).\n","\n","\n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensor.png\" width=\"600\" height=\"450\"/></center>\n"]},{"cell_type":"markdown","metadata":{"id":"rB6BbBz4lIfw"},"source":["<a id=section101></a>\n","### 1.1 Scalars (0D Tensors)"]},{"cell_type":"markdown","metadata":{"id":"s4_tvyFqlIfz"},"source":["- A **tensor** that contains only **one number** is called a **scalar** (or *scalar tensor*, or *0-dimensional tensor*, or *0D tensor*). \n","\n","\n","- In Numpy, a **float32** or **float64** number is a **scalar tensor** (or *scalar array*). \n","\n","\n","- You can display the **number of axes** of a Numpy tensor via the **`ndim`** attribute; a *scalar tensor* has **0 axes (`ndim == 0`)**. \n","\n","\n","- The *number of axes* of a tensor is also called its **rank**. \n","\n","\n","- Here’s a **Numpy scalar**:"]},{"cell_type":"code","metadata":{"id":"vkF1GqUolIf1"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBh1If62lIf7","outputId":"52665e3b-58c7-4e72-c60f-5a398dc0a988"},"source":["x = np.array(12)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(12)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"8D6phpMDlIgB","outputId":"19b244d9-4869-4882-eda8-440e5bd5f57c"},"source":["x.ndim"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"afTijKK6lIgH"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"CN0ZgdWSlIgJ"},"source":["<a id=section102></a>\n","### 1.2 Vectors (1D Tensors)"]},{"cell_type":"markdown","metadata":{"id":"diKxb08slIgL"},"source":["- An **array of numbers** is called a **vector**, or **1D tensor**. \n","\n","\n","- A *1D tensor* is said to have exactly **one axis**. \n","\n","\n","- Following is a **Numpy vector**:"]},{"cell_type":"code","metadata":{"id":"mfo9MM8NlIgM","outputId":"a30e4caf-9f52-4928-b9df-9c56855c10ac"},"source":["x = np.array([12, 3, 6, 14, 30])\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([12,  3,  6, 14, 30])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"QDRi0VeRlIgR","outputId":"3a6b0240-391a-4d96-a492-32eb7b83405f"},"source":["x.ndim"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"RzBafSMPlIgW"},"source":["- This **vector** has **five entries** and so is called a **5-dimensional vector**. \n","\n","\n","- **Don’t confuse** a **5D vector** with a **5D tensor**!\n","\n","- A **5D vector** has only **one axis** and has *five dimensions along its axis*, whereas a **5D tensor** has **five axes** (and may have *any number of dimensions along each axis*). \n","\n","\n","- **Dimensionality** can *denote either* the **number of entries along a specific axis** (as in the case of our *5D vector*) or the **number of axes in a tensor** (such as a *5D tensor*), which can be confusing at times. \n","\n","\n","- In the latter case, it’s technically more **correct** to talk about a **tensor of rank 5** (the *rank of a tensor* being the *number of axes*), but the ambiguous notation 5D tensor is common regardless."]},{"cell_type":"markdown","metadata":{"id":"6IlCPUN3lIgY"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"MABc7UnBlIgZ"},"source":["<a id=section103></a>\n","### 1.3 Matrices (2D Tensors)"]},{"cell_type":"markdown","metadata":{"id":"zUKs92KPlIga"},"source":["- An **array of vectors** is a **matrix**, or **2D tensor**. \n","\n","\n","- A **matrix has two axes** (often referred to **rows and columns**).\n","\n","- You can *visually interpret* a *matrix* as a **rectangular grid of numbers**.\n","\n","\n","- This is a **Numpy matrix**:"]},{"cell_type":"code","metadata":{"id":"DMssRi3ClIgd"},"source":["x = np.array([[5, 78, 2, 34, 0],\n","              [6, 79, 3, 35, 1],\n","              [7, 80, 4, 36, 2]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbzTqzn1lIgj","outputId":"70d85e32-d60b-4152-93aa-f1bfb8c503fd"},"source":["x.ndim"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"JHihXLhalIgn"},"source":["- The entries from the **first axis** are called the **rows**, and the entries from the **second axis** are called the **columns**. \n","\n","\n","- In the example:\n","\n","  - **[5, 78, 2, 34, 0]** is the **first row** of **`x`**,\n","  \n","  - **[5, 6, 7]** is the **first column**."]},{"cell_type":"markdown","metadata":{"id":"ydwTuLkdlIgp"},"source":["<a id=section104></a>\n","### 1.4 3D Tensors and Higher-Dimensional Tensors"]},{"cell_type":"markdown","metadata":{"id":"5NvzgWJGlIgr"},"source":["- If you **pack matrices** in a **new array**, you obtain a **3D tensor**, which you can *visually interpret* as a **cube of numbers**.\n","\n","\n","- Following is a **Numpy 3D tensor**:"]},{"cell_type":"code","metadata":{"id":"9mZRngRglIgt"},"source":["x = np.array([[[5, 78, 2, 34, 0],\n","               [6, 79, 3, 35, 1],\n","               [7, 80, 4, 36, 2]],\n","              [[15, 78, 32, 34, 80],\n","               [16, 79, 33, 35, 81],\n","               [17, 80, 34, 36, 82]],\n","              [[45, 78, 52, 34, 60],\n","               [46, 79, 53, 35, 71],\n","               [47, 80, 54, 36, 72]]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s013JW8vlIgy","outputId":"4fb7271f-b65e-4a14-a0a4-26c6abdde497"},"source":["x.ndim"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"FsaEJPG0lIg7"},"source":["- By *packing 3D tensors* in an *array*, you can **create** a **4D tensor**, and so on. \n","\n","\n","- In deep learning, you’ll generally **manipulate tensors** that are **0D to 4D**, although you may go up to **5D** if you process **video data**."]},{"cell_type":"markdown","metadata":{"id":"m1ZtAXPdlIg9"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"DnlOynXblIg_"},"source":["<a id=section105></a>\n","### 1.5 Key attributes of Tensors"]},{"cell_type":"markdown","metadata":{"id":"gt0HVgKUlIhA"},"source":["A tensor is defined by **three key attributes**:\n","\n","- **Number of axes (rank)** - For instance, a **3D tensor** has **three axes**, and a **matrix has two axes**.\n","\n","  - This is also called the tensor’s **`ndim`** in Python libraries such as Numpy.\n","\n","<br> \n","- **Shape** - This is a **tuple** of integers that describes how many **dimensions** the tensor has **along each axis**.\n","\n","  - For instance, the previous **matrix** example has shape **(3, 5)**, and the **3D tensor** example has shape **(3, 3, 5)**.\n","  \n","  - A **vector** has a **shape** with a **single element**, such as **(5,)**, whereas a **scalar** has an **empty shape, ()**.\n","\n","<br> \n","- **Data type** (usually called **`dtype`** in Python libraries) - This is the **type** of the **data contained** in the tensor; for instance, a tensor’s type could be **float32, uint8, float64**, and so on. \n","\n","  - On **rare** occasions, you may see a **char** tensor. \n","  \n","  - Note that **string tensors don’t exist in Numpy** (or in most other libraries), because tensors live in **preallocated, contiguous memory segments**: and **strings**, being **variable length**, would **preclude** the use of this implementation."]},{"cell_type":"markdown","metadata":{"id":"JoQ7gbMBlIhC"},"source":["- To make this more concrete, let’s look back at the data we processed in the above examples.\n","\n","\n","- We will display the **number of axes** of the tensor **`x`**, the **`ndim`** attribute:"]},{"cell_type":"code","metadata":{"id":"mUw4HMxolIhE","outputId":"505792f6-bc35-4d19-9918-b8667d4eca1a"},"source":["print(x.ndim)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uHgrykpwlIhY"},"source":["- Here’s its **shape**:"]},{"cell_type":"code","metadata":{"run_control":{"marked":false},"id":"KLcnkoAulIha","outputId":"6931c1a5-d142-445c-cad3-476d3ef28f56"},"source":["print(x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3, 3, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ekSspyWblIhg"},"source":["- And this is its **data type**, the **`dtype`** attribute:"]},{"cell_type":"code","metadata":{"id":"fVpKjHXglIh7","outputId":"10d94aeb-0ca4-4425-945e-39fd155767ee"},"source":["print(x.dtype)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["int32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mv8o915elIiP"},"source":["- So what we have here is a **3D tensor** of **32-bit integers**.\r\n","\r\n","- More precisely, it’s an **array of 3 matrices of 3 × 5 integers**. "]},{"cell_type":"markdown","metadata":{"id":"6PHcfTQFlIiW"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"gVqvhFSelIiX"},"source":["<a id=section106></a>\n","### 1.6 Manipulating tensors in Numpy"]},{"cell_type":"markdown","metadata":{"id":"yuM4xFMMlIib"},"source":["- **Selecting specific elements** in a tensor is called **tensor slicing**."]},{"cell_type":"code","metadata":{"id":"8k7dOonFlIie","outputId":"c116f7aa-86b9-4aa5-c6d5-41c70a32a3ca"},"source":["x[2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[45, 78, 52, 34, 60],\n","       [46, 79, 53, 35, 71],\n","       [47, 80, 54, 36, 72]])"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"YCvkHYP9lIjq"},"source":["- In the previous example, we selected a **specific digit** alongside the **first axis** using the syntax **`x[i]`**. \n","\n","\n","- Let’s look at the **tensor-slicing operations** you can do on **Numpy arrays**.\n","\n","  - The following example **selects matrices #0 to #2** (#2 isn’t included) and puts them in an **array of shape (2, 3, 5)**."]},{"cell_type":"code","metadata":{"id":"o5rLWUSRlIjv","outputId":"6c914fa3-b451-46da-f864-b58e462e6b2e"},"source":["my_slice = x[0:2]\n","print(my_slice.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2, 3, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BrM_DDrUlIj3"},"source":["- It’s equivalent to this more detailed notation, which specifies a **start index** and **stop index** for the **slice** along each tensor axis. \n","\n","\n","- Note that **`:`** is equivalent to **selecting** the **entire axis**."]},{"cell_type":"code","metadata":{"id":"bQwr2aG_lIku","outputId":"bd6819f8-956c-45aa-821d-04aa4669ca00"},"source":["my_slice = x[0:2, :, :]\n","my_slice.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 3, 5)"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"Azb2gJU4lIk5","outputId":"dc739955-14e1-4921-e32a-248ec21b6f25"},"source":["my_slice = x[0:2, 0:1, 0:4]\n","my_slice.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 1, 4)"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"9n_hGGbFlIlH"},"source":["- In general, you may **select between** any **two indices** along each tensor axis. \n","\n","\n","- For instance, in order to select **2 × 2 matrix** in the **bottom right corner** of **all** the **parent matrices**, you do this:"]},{"cell_type":"code","metadata":{"id":"k4BGW330lIlI","outputId":"c3900459-f5eb-485b-867c-e1e1b40599cf"},"source":["x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 5, 78,  2, 34,  0],\n","        [ 6, 79,  3, 35,  1],\n","        [ 7, 80,  4, 36,  2]],\n","\n","       [[15, 78, 32, 34, 80],\n","        [16, 79, 33, 35, 81],\n","        [17, 80, 34, 36, 82]],\n","\n","       [[45, 78, 52, 34, 60],\n","        [46, 79, 53, 35, 71],\n","        [47, 80, 54, 36, 72]]])"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"FwpefLZ7lIlO","outputId":"6a4cf15c-f334-4252-a932-5d91dbf699b8"},"source":["my_slice = x[:, 1:, 3:]\n","my_slice"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[35,  1],\n","        [36,  2]],\n","\n","       [[35, 81],\n","        [36, 82]],\n","\n","       [[35, 71],\n","        [36, 72]]])"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"3dt5GTEhlIlU"},"source":["- It’s also **possible** to **use** negative indices. \n","\n","\n","- Much like **negative indices** in Python lists, they **indicate** a **position relative to the end** of the **current axis**. \n","\n","\n","- In order to **select** the **central row** of **each parent matrix**, you do this:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VKrmntxNlIlV","outputId":"745be2b6-1fc3-463b-9ed7-9dfd6e3dcd74"},"source":["my_slice = x[:, 1:-1, :]\n","my_slice"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 6, 79,  3, 35,  1]],\n","\n","       [[16, 79, 33, 35, 81]],\n","\n","       [[46, 79, 53, 35, 71]]])"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"nFPBmHERlIlZ"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"qpfFDMxulIla"},"source":["<a id=section107></a>\n","### 1.7 The Notion of Data Batches"]},{"cell_type":"markdown","metadata":{"id":"ZB2qKuNJlIld"},"source":["- In general, the **first axis** (**axis 0**, because *indexing starts at 0*) in all **data tensors** you’ll come across in deep learning will be the **samples axis** (sometimes called the **samples dimension**). \n","\n","\n","- In addition, deep-learning models **don’t process** an **entire dataset at once**; rather, they **break** the **data** into **small batches**. \n","\n","<br> \n","- Concretely, here’s **one batch** of our **`x`**, with **batch size** of **1**:"]},{"cell_type":"code","metadata":{"id":"JQEXAAY1lIlf","outputId":"8a6b7a4a-e88c-4799-b18f-671bd2365c89"},"source":["batch = x[:1]\n","batch"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 5, 78,  2, 34,  0],\n","        [ 6, 79,  3, 35,  1],\n","        [ 7, 80,  4, 36,  2]]])"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"UcxzPNDUlIlm"},"source":["- And here’s the **next batch**:"]},{"cell_type":"code","metadata":{"id":"EOOICWpSlIlo","outputId":"661bd169-57bb-49fb-a565-9d96dbba48f9"},"source":["batch = x[1:2]\n","batch"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[15, 78, 32, 34, 80],\n","        [16, 79, 33, 35, 81],\n","        [17, 80, 34, 36, 82]]])"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"AUa868oblIlv"},"source":["- The **batch size** is the **difference between** the **two values** inputed in the **slice** above $(batch\\_size = 2 - 1 = 1)$.\n","\n","\n","- And the $n_{th} batch: batch = x[(1 * n):(1 * (n + 1))]$\n","\n","\n","- Here the **value before `:`** is the **multiple of $n$ and $batch\\_size$**.\n","\n","  - And, the **value after `:`** is the **multiple of $n+1$ and $batch\\_size$**."]},{"cell_type":"code","metadata":{"id":"29rLuduJlIlw","outputId":"c98a46d2-b18e-49e3-a6c8-3c423156140f"},"source":["# if n = 2\n","n = 2\n","batch = x[(1 * n):(1 * (n + 1))]\n","batch"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[45, 78, 52, 34, 60],\n","        [46, 79, 53, 35, 71],\n","        [47, 80, 54, 36, 72]]])"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"fGGEQuOrlIl0"},"source":["- When considering such a **batch tensor**, the **first axis (axis 0)** is called the **batch axis** or **batch dimension**. \n","\n","\n","- This is a term you’ll frequently encounter when using **Keras** and other deep-learning libraries."]},{"cell_type":"markdown","metadata":{"id":"dNRM9eeVlIl1"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"qokhukB9lIl2"},"source":["<a id=section108></a>\n","### 1.8 Real-world Examples of Data Tensors"]},{"cell_type":"markdown","metadata":{"id":"GxRpdAWElIl3"},"source":["- The **data** you’ll manipulate will almost always fall into one of the following **categories**:\n","<br><br> \n","  - **Vector data** - **2D tensors** of shape $(samples, features)$.\n","  <br><br> \n","  - **Timeseries data or Sequence data** - **3D tensors** of shape $(samples, timesteps, features)$.\n","  <br><br> \n","  - **Images** - **4D tensors** of shape $(samples, height, width, channels)$ or $(samples, channels, height, width)$.\n","  <br><br> \n","  - **Video** - **5D tensors** of shape $(samples, frames, height, width, channels)$ or $(samples, frames, channels, height, width)$.\n","  \n","<br>\n","\n","- Here **channels** refer to the **number of color channels**.\n","\n","  - For example, **gray scale images** have only a **single color channel**.\n","\n","<br>\n"," \n","<table bgcolor=\"white\">\n","  <tr text-align=\"left\">\n","    <th style=\"font-weight:bold; font-size:14px; text-align:center\">Timeseries Data</th>\n","    <th style=\"font-weight:bold; font-size:14px; text-align:center\">Image Data</th>\n","  </tr>\n","  <tr>\n","  <tr>\n","    <td><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/timeseries.png\" width=\"350\" height=\"350\"/></td>\n","    <td><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/imagedata.png\" width=\"350\" height=\"350\"/></td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"s6jZ4RkqlIl4"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"mVGbohCNlIl5"},"source":["<a id=section2></a>\n","## 2. Vector Data"]},{"cell_type":"markdown","metadata":{"id":"KryX_CIllImF"},"source":["- This is the **most common** case. \n","\n","- In such a dataset, **each single data point** can be encoded as a **vector**, and thus a **batch of data** will be encoded as a **2D tensor** (that is, an *array of vectors*), where the **first axis** is the **samples axis** and the **second axis** is the **features axis**."]},{"cell_type":"markdown","metadata":{"id":"KyFTYs60n_bZ"},"source":["- Let’s take a look at two **examples**:\r\n","<br><br>\r\n","  - An **actuarial** dataset of people, where we consider each person’s **age, ZIP code**, and **income**.\r\n","  \r\n","    - Each person can be characterized as a **vector of 3 values**, and thus an entire dataset of **100,000 people** can be **stored** in a **2D tensor** of shape **(100000, 3)**.\r\n","<br><br>\r\n","  - A dataset of text documents, where we represent **each document** by the **counts** of how many times each **word appears** in it (out of a dictionary of **20,000 common words**).\r\n","  \r\n","    - **Each document** can be encoded as a **vector** of **20,000** values (**one count per word** in the dictionary), and thus an entire dataset of **500 documents** can be **stored** in a tensor of shape **(500, 20000)**."]},{"cell_type":"markdown","metadata":{"id":"yGgGoUwwlImG"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"7mbVQqdVlImH"},"source":["<a id=section3></a>\n","## 3. Tensor Operations in a Nutshell"]},{"cell_type":"markdown","metadata":{"id":"T8OrvuKrlImI"},"source":["- Much as any computer program can be ultimately **reduced** to a **small set** of **binary operations** on binary inputs (**AND, OR, NOR,** and so on), all **transformations** learned by deep neural networks can be **reduced** to a handful of **tensor operations** applied to tensors of **numeric data**. \n","\n","\n","- For instance, it’s possible to **add** tensors, **multiply** tensors, and so on."]},{"cell_type":"markdown","metadata":{"id":"F0Xx4N6qlImJ"},"source":["<a id=section301></a>\n","### 3.1 Element-wise Operations"]},{"cell_type":"markdown","metadata":{"id":"SeScqRW8lImL"},"source":["- **Element-wise operations**: Operations that are **applied independently** to **each entry** in the **tensors** being considered.\n","\n","  - Examples are **relu** operation and **addition**.\n","\n","<br> \n","- This means these operations are **highly amenable to massively parallel implementations** (vectorized implementations, a term that comes from the vector processor supercomputer architecture from the 1970–1990 period). \n","\n","<br> \n","- If you want to write a naive Python implementation of an *element-wise operation*, you use a **for loop**, as in this naive implementation of an **element-wise addition**:"]},{"cell_type":"markdown","metadata":{"id":"YgHm7EfSlImP"},"source":["- #### Element-wise addition:\n","\n","    ```python\n","def naive_add(x, y):\n","    assert len(x.shape) == 2                  # x is a 2D Numpy tensor\n","    assert x.shape == y.shape\n","    x = x.copy()                              # Avoid overwriting the input tensor\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            x[i, j] += y[i, j]\n","    return x\n","    ```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WpSfK3mNlImQ"},"source":["- On the same principle, you can do element-wise **multiplication**, **subtraction**, and so on. \n","\n","\n","- In practice, when dealing with Numpy arrays, these **operations** are available as well **optimized built-in Numpy functions**, which themselves **delegate** the **heavy lifting** to a **Basic Linear Algebra Subprograms** (BLAS) implementation if you have one installed (which you should). \n","\n","\n","- **BLAS** are **low-level, highly parallel, efficient tensor-manipulation routines** that are typically **implemented** in **Fortran or C**. \n","\n","<br> \n","- So, in **Numpy**, you can do the following element-wise operation, and it will be **blazing fast**:"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":false},"id":"L2V_aPtZlImR"},"source":["- #### Element-wise addition:\n","\n","    **```z = x + y```**"]},{"cell_type":"markdown","metadata":{"id":"B_eWfsdklImU"},"source":["- #### Element-wise maximum operation (relu):\n","    **```z = np.maximum(z, 0.)```**"]},{"cell_type":"markdown","metadata":{"id":"vZiHNYU1lImV"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"RhBcw0rjlImW"},"source":["<a id=section302></a>\n","### 3.2 Broadcasting"]},{"cell_type":"markdown","metadata":{"id":"h8y8FlXXlImX"},"source":["- Our earlier naive implementation of **`naive_add`** only **supports** the **addition** of **2D tensors** with **identical** shapes.\n","\n","  - But **what happens** with **addition** when the **shapes** of the two tensors being added **differ**?\n","\n","<br> \n","- When possible, and if there’s no ambiguity, the **smaller tensor** will be **broadcasted** to **match** the **shape** of the **larger tensor**. "]},{"cell_type":"markdown","metadata":{"id":"raThH7dRlImY"},"source":["- **Broadcasting** consists of **two steps**:\n","  \n","  1. **Axes** (called **broadcast axes**) are **added** to the **smaller tensor** to **match** the **`ndim`** of the **larger tensor**.\n","<br><br>  \n","  2. The **smaller tensor** is **repeated alongside** these **new axes** to **match** the **full shape** of the **larger tensor**."]},{"cell_type":"markdown","metadata":{"id":"U6v4XnuBlImZ"},"source":["- Let’s look at a **concrete example**. \n","\n","  - Consider **`X`** with shape **(32, 10)** and **`y`** with shape **(10,)**. \n","  \n","  - First, we **add** an **empty first axis** to **`y`**, whose shape becomes **(1, 10)**. \n","  \n","  - Then, we **repeat `y` 32 times** alongside this **new axis**, so that we end up with a tensor **`Y`** with shape **(32, 10)**.\n","  \n","    - Here, **`Y[i, :] == y for i in range(0, 32)`**. \n","  \n","  - At this point, we can proceed to **add `X` and `Y`**, because they have the **same shape**.\n","\n","<br> \n","- In terms of implementation, **no new 2D tensor is created**, because that would be terribly **inefficient**.\n","\n","  - The **repetition operation** is entirely **virtual**: it happens at the **algorithmic level** rather than at the **memory level**.\n","  \n","  - But thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model. \n","\n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/broadcasting.png\" width=\"900\" height=\"650\"/></center>\n"]},{"cell_type":"markdown","metadata":{"id":"bsJOy99V6PK5"},"source":["- Here’s what a **naive implementation** would look like:\r\n","\r\n","    ```python\r\n","def naive_add_matrix_and_vector(x, y):\r\n","    assert len(x.shape) == 2                  # x is a 2D Numpy tensor\r\n","    assert len(y.shape) == 1                  # y is a Numpy vector\r\n","    assert x.shape[1] == y.shape[0]\r\n","    x = x.copy()                              # Avoid overwriting the input tensor\r\n","    for i in range(x.shape[0]):\r\n","        for j in range(x.shape[1]):\r\n","            x[i, j] += y[j]\r\n","    return x\r\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"k5ekB1mXlImb"},"source":["- With **broadcasting**, you can generally apply **two-tensor element-wise operations** if **one tensor** has shape $(a, b, … n, n + 1, … m)$ and the **other** has shape $(n, n + 1, … m)$.\n","\n","  - The broadcasting will then **automatically** happen for **axes a through $n - 1$**. \n","\n","<br>  "]},{"cell_type":"markdown","metadata":{"id":"Ve5Xk3tOzjyQ"},"source":["- The following example applies the **element-wise maximum operation** to **two tensors** of **different shapes** via **broadcasting**:\r\n","\r\n","    ```python\r\n","x = np.random.random((64, 3, 32, 10))        # x is a random tensor with shape (64, 3, 32, 10)\r\n","y = np.random.random((32, 10))               # y is a random tensor with shape (32, 10)\r\n","    ```\r\n","\r\n","    ```python\r\n","z = np.maximum(x, y)                         # The output z has shape (64, 3, 32, 10) like x\r\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"evQ7aDQwlImg"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"l3z8N-55lImj"},"source":["<a id=section303></a>\n","### 3.3 Tensor Dot"]},{"cell_type":"markdown","metadata":{"id":"0LWT6zn0lImk"},"source":["- The **`dot` operation**, also called a **tensor product** (**not** to be confused with an **element-wise product**) is the **most common**, **most useful tensor operation**. \n","\n","  - Contrary to element-wise operations, it **combines entries in the input tensors**.\n","\n","<br> \n","- An **element-wise product** is **done with** the $*$ **operator** in Numpy, Keras, Theano, and TensorFlow. \n","\n","<br> \n","- ___dot___ uses a *different syntax* in *TensorFlow*, but in both **Numpy and Keras** it’s **done using** the **standard dot operator**:  **```z = np.dot(x, y)```**"]},{"cell_type":"markdown","metadata":{"id":"ItL02IVXlImp"},"source":["- In **mathematical notation**, you’d note the **operation** with a **dot** (**.**):  **```z = x . y```**\n","\n","\n","- **Mathematically, what does the dot operation do?** \n","\n","<br> \n","- Let’s start with the **dot product** of **two vectors `x`** and **`y`**. It’s computed as follows:\n","\n","    ```python\n","def naive_vector_dot(x, y):\n","    assert len(x.shape) == 1                  \n","    assert len(y.shape) == 1              # x and y are Numpy vectors    \n","    assert x.shape[0] == y.shape[0]\n","    z = 0.\n","    for i in range(x.shape[0]):\n","        z += x[i] * y[i]\n","    return z\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"yw0QNsHOlImt"},"source":["- The **dot product** between two vectors is a **scalar** and that only **vectors with** the **same number of elements** are **compatible** for a dot product. \n","\n","<br> \n","- You can also take the dot product between a **matrix `x`** and a **vector `y`**, which **returns** a **vector** where the **coefficients** are the **dot products between `y`** and the **rows** of **`x`**.\n","\n","- You **implement** it as follows:\n","\n","    ```python\n","def naive_matrix_vector_dot(x, y):\n","    assert len(x.shape) == 2\n","    assert len(y.shape) == 1\n","    assert x.shape[1] == y.shape[0]\n","    z = np.zeros(x.shape[0])\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            z[i] += x[i, j] * y[j]\n","    return z\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"6q7SVVD2lIm8"},"source":["- Note that as soon as **one** of the two tensors has an **`ndim` greater than 1**, **dot is no longer symmetric**, which is to say that $dot(x, y)$ **isn’t** the **same** as $dot(y, x)$.\n","\n","- Of course, a dot product **generalizes** to tensors with an **arbitrary number of axes**.\n","\n","<br> \n","- The **most common applications** may be the dot product **between two matrices**. \n","  \n","  - You can take the **dot product** of **two matrices `x`** and **`y`**: $dot(x, y)$ if and only **if `x.shape[1] == y.shape[0]`**. \n","  \n","  - The **result** is a **matrix** with shape **`(x.shape[0], y.shape[1])`**, where the **coefficients** are the **vector products between** the **rows of `x`** and the **columns of `y`**. \n","\n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensordot2.png\" width=\"400\" height=\"250\"/></center>\n","\n","\n","- Here’s the **naive implementation**:\n","\n","    ```python\n","def naive_matrix_dot(x, y):\n","    assert len(x.shape) == 2\n","    assert len(y.shape) == 2\n","    assert x.shape[1] == y.shape[0]\n","    z = np.zeros((x.shape[0], y.shape[1]))\n","    for i in range(x.shape[0]):\n","        for j in range(y.shape[1]):\n","            row_x = x[i, :]\n","            column_y = y[:, j]\n","            z[i, j] = naive_vector_dot(row_x, column_y)\n","    return z\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"DRe7MkJulIm9"},"source":["- To understand **dot-product shape compatibility**, it helps to **visualize** the **input** and **output tensors** by **aligning them** as shown in figure:\n","\n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensordot3.png\" width=\"250\" height=\"250\"/></center>\n","<br> \n","\n","- **x**, **y**, and **z** are **pictured as rectangles** (literal boxes of coefficients). \n","\n","\n","- Because the **rows of x** and the **columns of y** must have the **same size**, it follows that the **width of x must match** the **height of y**. "]},{"cell_type":"markdown","metadata":{"id":"Fuvw0pfLlInB"},"source":["- You can take the dot product between **higher-dimensional tensors**, **following** the same **rules for shape compatibility** as outlined earlier for the 2D case:\n","<br><br>\n"," - $(a, b, c, d) . (d,) --> (a, b, c)$\n","<br><br>\n"," - $(a, b, c, d) . (d, e) --> (a, b, c, e)$\n","<br><br>\n"," - And so on.\n"," \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensordot.png\" width=\"450\" height=\"350\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"eRByKj0BlInD"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"LIa6z6hXlInF"},"source":["<a id=section304></a>\n","### 3.4 Tensor Reshaping"]},{"cell_type":"markdown","metadata":{"id":"620UNXhNlInM"},"source":["- A **third type** of **tensor operation** that’s essential to understand is **tensor reshaping**.\n","\n","- **Reshaping** a tensor means **rearranging** its **rows** and **columns** to **match** a **target shape**.\n","\n","- Naturally, the **reshaped tensor** has the **same total number of coefficients** as the **initial tensor**. \n","\n","<br> \n","- **Reshaping** is best understood via simple **examples**:"]},{"cell_type":"code","metadata":{"id":"_jJCDP6olInU"},"source":["x = np.array([[0., 1.],\n","              [2., 3.],\n","              [4., 5.]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"391Tp9RllInh","outputId":"57fa3906-16c7-47a3-c14b-4a0e776238ee"},"source":["print(x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tp1dOOvVlInr","outputId":"3a69d420-f8f8-4b4c-b271-8c41dd8790b3"},"source":["x = x.reshape((6, 1))\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.],\n","       [1.],\n","       [2.],\n","       [3.],\n","       [4.],\n","       [5.]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"U1R1h2PalInu","outputId":"3ab65dbd-db06-47b7-e4bd-e8239e8830ca"},"source":["x = x.reshape((2, 3))\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 2.],\n","       [3., 4., 5.]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"SRXph4EulIny"},"source":["- A **special case** of **reshaping** that’s commonly encountered is **transposition**. \n","\n","\n","- **Transposing a matrix** means **exchanging its rows** and its **columns**, so that **`x[i, :]` becomes `x[:, i]`**:"]},{"cell_type":"code","metadata":{"id":"MtMiO5vSlIn0","outputId":"bc1cf364-68d6-4642-b3e3-789c3005a448"},"source":["x = np.zeros((300, 20))                   # Creates an all-zeros matrix of shape (300, 20)\n","x = np.transpose(x)\n","print(x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(20, 300)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GQoqWCenlIn7"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"AinjIDRXlIn8"},"source":["<a id=section305></a>\n","### 3.5 A Geometric Interpretation of Deep Learning"]},{"cell_type":"markdown","metadata":{"id":"2e4A-yh_lIn9"},"source":["- You just learned that **neural networks consist** entirely of **chains** of **tensor operations** and that all of these **tensor operations** are just **geometric transformations** of the **input data**. \n","\n","- It follows that you can **interpret** a neural network as a **very complex geometric transformation in a high-dimensional space**, **implemented via** a **long series of simple steps**. \n","\n","<br> \n","- **In 3D**, the following **mental image** may prove **useful**. \n","\n"," - Imagine **two sheets** of colored paper: **one red** and **one blue**. \n","\n"," - Put one on **top of** the other. \n"," \n"," - Now **crumple** them together into a **small ball**. \n"," \n"," - That **crumpled paper ball** is your **input data**, and **each sheet** of paper is a **class of data** in a **classification** problem. \n"," \n"," - What a neural network (or any other machine-learning model) is meant to do is **figure out a transformation** of the **paper ball** that would **uncrumple it**, so as to make the two **classes cleanly separable** again. \n"," \n"," - With **deep learning**, this would be implemented as a series of **simple transformations** of the **3D space**, such as those you could apply on the paper ball with your **fingers**, **one movement at a time**.\n","<br><br>  \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/deep_learning_interpretation.png\" width=\"700\" height=\"350\"/></center>\n","<br> "]},{"cell_type":"markdown","metadata":{"id":"lO85ImXFlIn-"},"source":["- Uncrumpling paper balls is what machine learning is about: **finding neat representations** for **complex, highly folded data manifolds**. \n","\n","- At this point, you should have a pretty good intuition as to **why deep learning excels** at this.\n","\n","  - It takes the approach of **incrementally decomposing** a **complicated geometric transformation** into a **long chain of elementary ones**.\n","  \n","  - This is pretty much the strategy a **human** would **follow** to **uncrumple** a paper ball. \n","\n","- **Each layer** in a deep network **applies** a **transformation** that **disentangles** the **data** a little and a **deep stack** of layers makes *tractable* an **extremely complicated disentanglement** process."]},{"cell_type":"markdown","metadata":{"id":"TJg5-bTxlIn_"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"CBL46qIylIoA"},"source":["<a id=section4></a>\n","## 4. Basic Maths for Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"PGFLISlAlIoB"},"source":["- Each neural layer **transforms** its **input data** as follows:\r\n","\r\n","  $output = relu(dot(W, input) + b)$"]},{"cell_type":"markdown","metadata":{"id":"fi7WRLdRlIoD"},"source":["- In this expression, $W$ and $b$ are **tensors** that are **attributes** of the layer. \n","\n","  - They’re called the **weights** or **trainable parameters** of the layer (the **kernel** and **bias** attributes, respectively). \n"," \n","  - These **weights contain** the **information learned** by the **network** from **exposure** to training data."]},{"cell_type":"markdown","metadata":{"id":"DIOKp2J9lIoE"},"source":["- Initially, these **weight matrices** are **filled** with **small random values** (a step called **random initialization**). \n","\n","- Of course, there’s no reason to expect that $relu(dot(W, input) + b)$, when $W$ and $b$ are **random**, will **yield** any **useful representations**.\n","\n","  - The resulting representations are **meaningless**, but they’re a **starting point**. \n","\n","- What comes next is to **gradually adjust** these **weights**, based on a **feedback signal**.\n","\n","  - This **gradual adjustment**, also called **training**, is basically the **learning** that machine learning is all about. \n","\n","<br> \n","- This happens within what’s called a **training loop**, which works as follows. **Repeat** these **steps** in a **loop**, as long as necessary:\n","\n","  1. **Draw** a **batch of training samples `x`** and **corresponding targets `y`**.\n","<br><br>\n","  2. **Run** the **network on x** (a step called the **forward pass**) to obtain **predictions `y_pred`**.\n","<br><br>\n","  3. **Compute** the **loss** of the network on the **batch**, a **measure** of the **mismatch between `y_pred`** and **`y`**.\n","<br><br>\n","  4. **Update all weights** of the **network** in a way that slightly **reduces** the **loss** on this **batch**.\n"," \n"," <br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/gradient_descent.png\" width=\"600\" height=\"400\"/></center>\n","<br> "]},{"cell_type":"markdown","metadata":{"id":"OajuEcSWlIoE"},"source":["- You’ll eventually end up with a **network** that has a **very low loss** on its **training data**: a **low mismatch between predictions `y_pred`** and **expected targets `y`**. \n","\n","\n","- The network has **“learned” to map** its **inputs to correct targets**. \n","\n","\n","- From afar, it may look like magic, but when you **reduce** it to **elementary steps**, it turns out to be **simple**."]},{"cell_type":"markdown","metadata":{"id":"HOnBQJFDlIoF"},"source":["- **Step 1** sounds easy enough - just **I/O code**.\n","\n","- **Steps 2 and 3** are merely the **application of** a handful of **tensor operations**, so you could implement these steps purely from what you learned in the previous section. \n","\n","- The **difficult** part is **step 4**: **Updating the network's weights**.\n","\n","  - Given an **individual weight coefficient** in the network, how can you **compute** whether the **coefficient** should be **increased** or **decreased**, and **by how much?**\n","\n","<br>\n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/GD.png\" width=\"600\" height=\"450\"/></center>\n","<br> \n","\n","- One **naive** solution would be to **freeze all weights** in the network **except** the **one scalar coefficient** being considered, and **try different values** for this **coefficient**.\n","\n","  - But such an approach would be horribly **inefficient**, because you’d need to **compute two forward passes** (which are **expensive**) for **every individual coefficient** (of which there are many, usually **thousands** and sometimes **up to millions**). "]},{"cell_type":"markdown","metadata":{"id":"eM5OOKg_lIoJ"},"source":["- A much **better approach** is to take advantage of the fact that all **operations** used in the network are **differentiable**, and **compute** the **gradient** of the **loss** with regard to the network’s **coefficients**. \n","\n","\n","- You can then **move** the **coefficients** in the **opposite direction** from the **gradient**, thus **decreasing** the **loss**."]},{"cell_type":"markdown","metadata":{"id":"bITLv6YclIoK"},"source":["<a id=section401></a>\n","### 4.1 What’s a Derivative?"]},{"cell_type":"markdown","metadata":{"id":"qqanIzChlIoS"},"source":["- Consider a continuous, smooth function $f(x) = y$, mapping a real number $x$ to a new real number $y$. \n","<br> \n","  - Because the function is **continuous**, a **small change** in $x$ can only **result** in a **small change** in $y$, that’s the intuition behind continuity. \n","<br><br>  \n","  - Let’s say you **increase** $x$ by a **small factor** $epsilon\\_x$: this **results** in a **small** $epsilon\\_y$ **change** to $y$:\n","  \n","    $f(x + epsilon\\\\_x) = y + epsilon\\\\_y$"]},{"cell_type":"markdown","metadata":{"id":"VpcXj9K8lIoU"},"source":["- In addition, because the **function** is **smooth** (its *curve doesn’t have any abrupt angles*), when $epsilon\\_x$ is **small** enough, around a certain **point** $p$.\r\n","\r\n","  - It’s possible to **approximate f** as a **linear function** of **slope a**,  so that $epsilon\\_y$ **becomes** $a * epsilon\\_x$:\r\n","\r\n","    $f(x + epsilon\\\\_x) = y + a * epsilon\\\\_x$"]},{"cell_type":"markdown","metadata":{"id":"_xudGGDflIoW"},"source":["- Obviously, this linear approximation is **valid** only when $x$ is **close** enough to $p$. \n","<br><br> \n","  - The **slope** $a$ is called the **derivative** of $f$ in $p$. \n","<br><br> \n","  - If $a$ is **negative**, it means a **small change** of $x$ around $p$ will **result** in a **decrease** of $f(x)$. \n","<br><br> \n","  - And if $a$ is **positive**, a **small change** in $x$ will **result** in an **increase** of $f(x)$. \n","<br><br>   \n","  - Further, the **absolute value** of $a$ (the **magnitude** of the **derivative**) tells you **how quickly** this **increase or decrease will happen**.\n","  \n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/derivative.png\" width=\"400\" height=\"350\"/></center>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"h8Gw_zKGlIoX"},"source":["- For every **differentiable function** $f(x)$ (differentiable means **“can be derived”**: for example, smooth, continuous functions can be derived), there **exists** a **derivative function** $f'(x)$ that **maps values** of $x$ to the **slope** of the **local linear approximation** of $f$ in those **points**. \n","\n","<br> \n","- For instance, the **derivative** of $cos(x$) is $-sin(x)$, the **derivative** of $f(x) = a * x$ is $f'(x) = a$, and so on.\n","\n","<br> \n","- If you’re trying to **update** $x$ by a **factor** $epsilon\\_x$ in order **to minimize** $f(x)$, and you **know** the **derivative** of $f$, then your job is done.\n","\n","  - The **derivative** completely **describes** how $f(x)$ **evolves** as you **change** $x$.\n","  \n","  - If you want to **reduce** the **value** of $f(x)$, you just need to **move** $x$ a little in the **opposite direction** from the **derivative**."]},{"cell_type":"markdown","metadata":{"id":"nS-QrfE0oxnh"},"source":["- To learn more about **Derivatives**, please refer to the below video:\r\n","\r\n","  - [**Derivative as a Concept**](https://www.youtube.com/watch?v=N2PpRnFqnqY)\r\n","\r\n","  - [**Directional Derivative**](https://www.youtube.com/watch?v=N_ZRcLheNv0)\r\n","\r\n","  - [**Directional Derivatives and Slope**](https://www.youtube.com/watch?v=4tdyIGIEtNU)"]},{"cell_type":"markdown","metadata":{"id":"pdqLEAfblIoY"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Ytpj1_-LlIoZ"},"source":["<a id=section402></a>\n","### 4.2 Derivative of a Tensor Operation: the Gradient"]},{"cell_type":"markdown","metadata":{"id":"JkqQMnRWlIoa"},"source":["- A **gradient** is the **derivative** of a **tensor operation**. \n","\n","- It’s the **generalization** of the concept of **derivatives** to **functions of multidimensional inputs**: that is, to **functions** that take **tensors** as **inputs**.\n","\n","<br> \n","- Consider an **input vector** $x$, a **matrix** $W$, a **target** $y$, and a **loss function** $loss$.\n","\n","- You can **use** $W$ to **compute** a **target candidate** $y\\_pred$, and **compute** the **loss**, or mismatch, between the target candidate $y\\_pred$ and the **target** $y$:\n","\n","    $y\\\\_pred = dot(W, x)$\n","\n","    $loss\\\\_value = loss(y\\\\_pred, y)$"]},{"cell_type":"markdown","metadata":{"id":"CS365lBolIoe"},"source":["- If the **data inputs** $x$ and $y$ are **frozen**, then this can be **interpreted** as a **function mapping values** of $W$ to $loss$ values:\r\n","\r\n","    $loss\\\\_value = f(W)$"]},{"cell_type":"markdown","metadata":{"id":"FOr3_C7-lIof"},"source":["- Let’s say the **current value** of $W$ is $W0$. \n","<br><br>\n"," - Then the **derivative** of $f$ in the **point** $W0$ is a tensor $gradient(f)(W0)$ with the **same shape** as $W$.\n"," \n"," - Here **each coefficient** $gradient(f)(W0)[i, j]$ indicates the **direction** and **magnitude** of the **change** in $loss\\_value$ you observe when **modifying** $W0[i, j]$. \n","\n"," - That tensor $gradient(f)(W0)$ is the **gradient** of the function $f(W) = loss\\_value$ in $W0$.\n","\n","<br> \n","- The **derivative** of a function $f(x)$ of a **single coefficient** can be **interpreted** as the **slope** of the **curve** of $f$.\n","\n","  - Likewise, $gradient(f)(W0)$ can be interpreted as the **tensor describing** the **curvature** of $f(W)$ around $W0$.\n","\n","<br> \n","- For this reason, in much the same way that, for a function $f(x)$, you can **reduce** the value of $f(x)$ by **moving** $x$ a little in the **opposite direction** from the **derivative**, with a function $f(W)$ of a tensor, you can **reduce** $f(W)$ by **moving** $W$ in the **opposite direction** from the **gradient**: \n","<br><br>\n","  - For example, $W1 = W0 - step * gradient(f)(W0)$ (where **step** is a **small scaling factor**). \n","<br><br>  \n","  - That means going **against** the **curvature**, which intuitively should put you **lower** on the **curve**. \n","<br><br>  \n","  - Note that the **scaling factor step** is **needed** because $gradient(f)(W0)$ only **approximates** the **curvature** when you’re **close to W0**, so you don’t want to get **too far from W0**."]},{"cell_type":"markdown","metadata":{"id":"kxXtCZbBnU1v"},"source":["- To learn more about **Gradients**, please refer to the below videos:\r\n","\r\n","  - [**Gradient**](https://www.youtube.com/watch?v=tIpKfDc295M)\r\n","\r\n","  - [**Gradient and Graphs**](https://www.youtube.com/watch?v=_-02ze7tf08)\r\n","\r\n","  - [**Why the gradient is the direction of steepest ascent?**](https://www.youtube.com/watch?v=TEB2z7ZlRAw)"]},{"cell_type":"markdown","metadata":{"id":"m6k7EFaYlIoj"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"NsgNijmClIom"},"source":["<a id=section403></a>\n","### 4.3 Stochastic Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"s3msJ562lIoo"},"source":["- Given a **differentiable function**, it’s theoretically possible to **find** its **minimum** analytically.\n","\n","  - It’s known that a **function’s minimum** is a **point** where the **derivative** is $0$.\n","\n","  - So all you have to do is **find all** the **points** where the **derivative** goes to $0$ and **check** for **which** of these **points** the **function** has the **lowest value**.\n","\n","<br> \n","- Applied to a neural network, that means **finding analytically** the **combination** of **weight values** that **yields** the **smallest possible loss function**. \n","<br><br>\n","  - This can be done by **solving** the equation $gradient(f)(W) = 0$ for $W$. \n","<br><br>  \n","  - This is a **polynomial equation** of $N$ **variables**, where $N$ is the **number of coefficients** in the network. \n","<br><br>  \n","  - Although it would be **possible** to **solve** such an equation for $N = 2$ or $N = 3$, doing so is **intractable** for **real neural networks**, where the **number of parameters** is never less than a **few thousand** and can often be several **tens of millions**."]},{"cell_type":"markdown","metadata":{"id":"2b1_w39klIot"},"source":["- Instead, you can **use** the **four-step algorithm** outlined at the beginning of this section: **modify** the **parameters little by little** based on the **current loss** value on a **random batch** of data. \n","\n","- Because you’re dealing with a **differentiable function**, you can **compute** its **gradient**, giving you an **efficient** way to **implement step 4**. \n","\n","<br> \n","- If you **update** the **weights** in the **opposite direction** from the **gradient**, the **loss** will be a **little less every time**:\n","<br> \n","  1. **Draw** a **batch of training samples** $x$ and **corresponding targets** $y$.\n","<br><br>    \n","  2. **Run** the **network** on $x$ to **obtain predictions** $y\\_pred$.\n","<br><br>    \n","  3. **Compute** the **loss** of the network on the **batch**, a **measure** of the **mismatch** between $y\\_pred$ and $y$.\n","<br><br>    \n","  4. **Compute** the **gradient** of the **loss** with regard to the **network’s parameters** (a **backward pass**).\n","<br><br>   \n","  5. **Move** the **parameters** a **little** in the **opposite direction** from the **gradient**. \n","<br><br>   \n","    - For example $W -= step * gradient$, thus **reducing** the **loss** on the **batch** a bit.\n","\n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/SGD.png\" width=\"600\" height=\"400\"/></center>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"WWGI9Z22lIou"},"source":["- This is called **mini-batch stochastic gradient descent** (**minibatch SGD**). \n","\n","\n","- The term **stochastic** refers to the fact that **each batch** of data is **drawn at random** (**stochastic** is a **scientific synonym** of **random**)."]},{"cell_type":"markdown","metadata":{"id":"8WcbmmOllIoy"},"source":["- It’s important to **pick** a **reasonable** value for the **step factor** (a.k.a. **learning rate**). \n","<br><br> \n","  - If it’s **too small**, the **descent down** the **curve** will take **many iterations**, and it could get **stuck** in a **local minimum**. \n","<br><br>   \n","  - If **step** is **too large**, your **updates** may end up taking you to **completely random locations** on the **curve**. \n","  \n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/learning_rate.png\" width=\"600\" height=\"350\"/></center>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"jSkB5qZKlIoz"},"source":["- Note that a **variant** of the **mini-batch SGD algorithm** would be to **draw a single sample** and **target** at **each iteration**, rather than **drawing** a **batch** of data.\n","\n","  - This would be **true SGD** (as **opposed to mini-batch SGD**). \n","\n","\n","- Alternatively, going to the **opposite extreme**, you could **run every step on all data available**, which is called **batch SGD**.\n","\n","  - **Each update** would then be **more accurate**, but far **more expensive**. \n","\n","\n","- The **efficient compromise** between these two **extremes** is to **use mini-batches** of **reasonable size**.\n","\n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/SGD_2D5.jpg\" width=\"800\" height=\"450\"/></center>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"DZyrhMwMlIo0"},"source":["- Additionally, there exist **multiple variants of SGD** that **differ** by taking into account **previous weight updates** when **computing** the **next weight update**, rather than just looking at the **current value** of the **gradients**. \n","\n","\n","- There is, for instance, **SGD with momentum**, as well as **Adagrad**, **RMSProp**, and several others.\n","\n","  - Such variants are known as **optimization methods** or **optimizers**. \n","\n","\n","- In particular, the concept of **momentum**, which is used in many of these variants, deserves your attention. \n","\n","<br> \n","- **Momentum** addresses **two issues** with **SGD**: **convergence speed** and **local minima**.\n","\n","  \n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/minimas2.png\" width=\"600\" height=\"400\"/></center>\n","<br><br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/minimas.png\" width=\"700\" height=\"450\"/></center>\n","<br>\n","\n","<br><br> \n","  - As you can see, around a certain **parameter value**, there is a **local minimum**: around that point, **moving left** would result in the **loss increasing**, but so would **moving right**. \n","<br><br>   \n","  - If the **parameter** under consideration were being **optimized via SGD** with a **small learning rate**, then the **optimization** process would get **stuck** at the **local minimum** instead of making its way to the **global minimum**."]},{"cell_type":"markdown","metadata":{"id":"9L7x2O5dlIo3"},"source":["- You can **avoid** such **issues** by **using momentum**, which draws inspiration from physics. \n","<br><br> \n","  - A useful mental image here is to think of the **optimization** process as a **small ball rolling down** the **loss curve**. \n","\n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/momentum.png\" width=\"600\" height=\"400\"/></center>\n","<br>\n","\n","  - If it has **enough momentum**, the **ball won’t** get **stuck** in a **ravine** and will **end** up at the **global minimum**. \n","<br><br>  \n","  - **Momentum** is implemented by **moving** the **ball** at **each step** based not only on the **current slope value** (*current acceleration*) but also on the **current velocity** (*resulting from past acceleration*). \n","<br><br>   \n","  - In practice, this means **updating** the **parameter** $w$ based not only on the **current gradient value** but **also** on the **previous parameter update**, such as in this naive implementation:\n","\n","    ```python\n","past_velocity = 0.\n","momentum = 0.1                                                           # Constant momentum factor\n","while loss > 0.01:                                                       # Optimization loop \n","    w, loss, gradient = get_current_parameters()\n","    velocity = past_velocity * momentum + learning_rate * gradient\n","    w = w + momentum * velocity - learning_rate * gradient\n","    past_velocity = velocity\n","    update_parameter(w)\n","    ```"]},{"cell_type":"markdown","metadata":{"id":"bFb0RMzolIo6"},"source":["<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/momentum1.png\" width=\"500\" height=\"500\"/></center>\n","<br>\n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/momentum2.png\" width=\"600\" height=\"400\"/></center>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"oUvo4GhElIo7"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"pWFVo7ctlIo9"},"source":["<a id=section404></a>\n","### 4.4 Chaining Derivatives: the Backpropagation Algorithm"]},{"cell_type":"markdown","metadata":{"id":"r11NiFldlIo-"},"source":["- In the previous algorithm, we casually **assumed** that because a **function is differentiable**, we can explicitly **compute its derivative**. \n","\n","- In practice, a neural network function consists of **many tensor operations chained together**, **each** of which has a **simple, known derivative**. \n","\n","\n","- For instance, this is a network $f$ **composed** of **three tensor operations**, $a$, $b$, and $c$, with **weight matrices** $W1$, $W2$, and $W3$:\n","\n","    $f(W1, W2, W3) = a(W1, b(W2, c(W3)))$"]},{"cell_type":"markdown","metadata":{"id":"V62NNPFwlIpb"},"source":["- **Calculus** tells us that such a **chain of functions** can be **derived using** the following **identity**, called the **chain rule**:\r\n","\r\n","  $f(g(x)) = f'(g(x)) * g'(x)$"]},{"cell_type":"markdown","metadata":{"id":"-xIPkTdZlIpe"},"source":["- Applying the **chain rule** to the **computation** of the **gradient values** of a neural network gives rise to an algorithm called **Backpropagation** (also sometimes called **reverse-mode differentiation**). \n","\n","\n","- Backpropagation **starts** with the **final loss value** and **works backward** from the **top layers** to the **bottom layers**, **applying** the **chain rule** to **compute** the **contribution** that **each parameter** had in the **loss value**.\n","\n","<br> \n","<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/backpropagation.png\" width=\"500\" height=\"450\"/></center>\n","<br>\n","\n","- Nowadays, and for years to come, people will implement networks in modern frameworks that are capable of **symbolic differentiation**, such as **TensorFlow**. \n","<br><br> \n","  - This means that, **given a chain of operations** with a **known derivative**, they can **compute** a **gradient function** for the **chain** (*by applying the chain rule*) that **maps network parameter values** to **gradient values**. \n","<br><br> \n","  - When you have access to such a function, the **backward pass** is **reduced** to a **call** to this **gradient function**. \n","<br><br> \n","  - Thanks to **symbolic differentiation**, you’ll **never** have to **implement** the **Backpropagation algorithm by hand**. \n","<br><br>  \n","  - For this reason, we won’t waste your time and your focus on deriving the exact formulation of the Backpropagation algorithm.\n","<br><br>  \n","  - All you need is a good understanding of **how gradient-based optimization works**."]}]}